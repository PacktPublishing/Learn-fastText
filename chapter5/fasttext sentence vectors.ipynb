{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and sentence vectors\n",
    "\n",
    "Natively fasttext only computes sentence vectors during supervised learning.\n",
    "\n",
    "Depending on the task, simply average word embeddings of all words in the sentence should suffc. (If doing so, you should normalize the word vectors first, so that they all have a norm equal to one.)\n",
    "\n",
    "According to Kenter et al. 2016, this approach \"has proven to be a strong baseline or feature across a multitude of tasks\", such as short text similarity tasks.\n",
    "\n",
    "However, according to Le and Mikolov, this method performs poorly for sentiment analysis tasks and/or long texts, because it \"loses the word order in the same way as the standard bag-of-words models do\" and \"fail[s] to recognize many sophisticated linguistic phenomena, for instance sarcasm\".\n",
    "\n",
    "In fasttext unsupervised case they average the normalized word embeddings (not sure what you mean by element-wise normalization, but they use plain L2 normalization of the vector as you can see in Vector::norm() of fastText/src/vector.cc). https://github.com/facebookresearch/fastText/blob/d647be03243d2b83d0b4659a9dbfb01e1d1e1bf7/src/vector.cc#L28\n",
    "\n",
    "So what you can do is that take the word vectors. COmpute the l2 normalisation and find the documents vector for this case. Please keep in mind that I do not really recommend this method right now because of unavailable benchmarks right now so use this method right now at your own discretion. The aim is to show you how such methods can be coded and hence with experience you will be able to create and implement your own methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence, ft_model):\n",
    "    sentence = 'night is black'\n",
    "    sentence = sentence.lower().split()\n",
    "    if len(sentence) == 1:\n",
    "        return ft_model[sentence[0]]\n",
    "    vecs = [ft_model.wv[x] for x in sentence]\n",
    "    X = np.asarray(vecs, dtype=np.float) # Float is needed.\n",
    "    X_normalized = preprocessing.normalize(X, norm='l2') # l2-normalize the samples (rows). \n",
    "    return np.mean(X_normalized, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download if file not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki.simple.zip\n",
      "  inflating: wiki.simple.vec         \n",
      "  inflating: wiki.simple.bin         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'wiki.simple.zip' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -nc https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.zip\n",
    "unzip -o wiki.simple.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fasttext model and compute the sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model ...\n",
      "\n",
      "\n",
      "SUM\n",
      "dot(vec1,vec2) 0.45278367773655714\n",
      "norm(p1) 0.672892025317998\n",
      "norm(p2) 0.672892025317998\n",
      "dot((norm)vec1,norm(vec2)) 0.4527836777365572\n",
      "cosine(vec1,vec2) 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word2vec model ...\\n\")\n",
    "modelpath = \"wiki.simple.bin\"\n",
    "ft_model = FastText.load_fasttext_format(modelpath)\n",
    "pattern_1 = 'founder and ceo'\n",
    "pattern_2 = 'co-founder and former chairman'\n",
    "\n",
    "p1 = sentence_vector(pattern_1, ft_model)\n",
    "p2 = sentence_vector(pattern_2, ft_model)\n",
    "print (\"\\nSUM\")\n",
    "print (\"dot(vec1,vec2)\", np.dot(p1,p2))\n",
    "print (\"norm(p1)\", norm(p1))\n",
    "print (\"norm(p2)\", norm(p2))\n",
    "print (\"dot((norm)vec1,norm(vec2))\", np.dot(norm(p1),norm(p2)))\n",
    "print (\"cosine(vec1,vec2)\", np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUM\n",
      "dot(vec1,vec2) 0.45278367773655714\n",
      "norm(p1) 0.672892025317998\n",
      "norm(p2) 0.672892025317998\n",
      "dot((norm)vec1,norm(vec2)) 0.4527836777365572\n",
      "cosine(vec1,vec2) 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "\n",
    "p1 = sentence_vector(sentence_obama, ft_model)\n",
    "p2 = sentence_vector(sentence_president, ft_model)\n",
    "print (\"\\nSUM\")\n",
    "print (\"dot(vec1,vec2)\", np.dot(p1,p2))\n",
    "print (\"norm(p1)\", norm(p1))\n",
    "print (\"norm(p2)\", norm(p2))\n",
    "print (\"dot((norm)vec1,norm(vec2))\", np.dot(norm(p1),norm(p2)))\n",
    "print (\"cosine(vec1,vec2)\",     np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9839733449425827"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Movers distance\n",
    "sentence_obama = 'founder and ceo'.lower().split()\n",
    "sentence_president = 'co-founder and former chairman'.lower().split()\n",
    "\n",
    "# Remove their stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]\n",
    "\n",
    "# Compute WMD.\n",
    "distance = ft_model.wv.wmdistance(sentence_obama, sentence_president)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosing to be 1 means that the angle between them is 0. which means have the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.969142709901333\n"
     ]
    }
   ],
   "source": [
    "# Word Movers distance\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "\n",
    "# Remove their stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]\n",
    "\n",
    "# Compute WMD.\n",
    "distance = ft_model.wv.wmdistance(sentence_obama, sentence_president)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note the supervised case takes the vector of EOS : </s> also as a bias. That is not done here. If you want you can include that in your analysis."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
